{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJV5AJmkyRICj0r6cAWjzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyaKota22/PROJECTS-PRACTICE/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TSmXjpMK5br"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/dataset/UCMerced_LandUse/Images'  # Directory containing the 21 folders\n",
        "img_width, img_height = 150, 150  # Desired image dimensions\n",
        "\n",
        "# List all the class labels\n",
        "class_labels = os.listdir(data_dir)\n",
        "\n",
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each folder\n",
        "for class_label in class_labels:\n",
        "    folder_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_files = os.listdir(folder_path)\n",
        "\n",
        "        # Load and resize the images, and store the labels\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.resize(img, (img_width, img_height))\n",
        "            images.append(img)\n",
        "            labels.append(class_label)\n",
        "\n",
        "# Convert the images and labels to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Perform label encoding on the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Perform one-hot encoding on the labels\n",
        "num_classes = len(class_labels)\n",
        "labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.arange(len(images))\n",
        "np.random.shuffle(indices)\n",
        "images = images[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Perform data normalization\n",
        "images = images.astype('float32') / 255.0\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_split = 0.7  # Percentage of data for training\n",
        "val_split = 0.15  # Percentage of data for validation\n",
        "test_split = 0.15  # Percentage of data for testing\n",
        "\n",
        "num_samples = len(images)\n",
        "num_train_samples = int(train_split * num_samples)\n",
        "num_val_samples = int(val_split * num_samples)\n",
        "\n",
        "x_train = images[:num_train_samples]\n",
        "y_train = labels[:num_train_samples]\n",
        "\n",
        "x_val = images[num_train_samples:num_train_samples + num_val_samples]\n",
        "y_val = labels[num_train_samples:num_train_samples + num_val_samples]\n",
        "\n",
        "x_test = images[num_train_samples + num_val_samples:]\n",
        "y_test = labels[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Data preprocessing completed.\")\n",
        "print(\"Number of training samples:\", len(x_train))\n",
        "print(\"Number of validation samples:\", len(x_val))\n",
        "print(\"Number of testing samples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "xnAzO0h7LM6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained VGG16 model\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers of the VGG16 model\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "FB7aIAnZLM20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new model by combining the VGG16 model and the CNN model\n",
        "combined_model = Sequential()\n",
        "combined_model.add(vgg_model)\n",
        "combined_model.add(Flatten())\n",
        "combined_model.add(Dense(128, activation='relu'))\n",
        "combined_model.add(Dropout(0.5))\n",
        "combined_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the combined model\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the combined model\n",
        "combined_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = combined_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "Xhl2Fb3dLSa0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}